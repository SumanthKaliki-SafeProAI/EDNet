{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49d8c8ee-5d61-495e-811e-a944bf988ece",
   "metadata": {},
   "source": [
    "## Model Evaluation & Export\n",
    "\n",
    "This notebook demonstrates how to export the model to TensorRT for NVIDIA GPUs, OpenVINO for Intel CPUs, and ONNX, a versatile format supported by many different frameworks and devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10ec188-5d42-4356-b4ac-b6b9b6387176",
   "metadata": {},
   "source": [
    "### 1. Load a pretrained model\n",
    "\n",
    "Choose from tiny, small, nano, medium, big, large, and xlarge according to your hardware capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afa9f3f0-0bd0-4a75-a05c-15dd5aacffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ednet import EDNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4192256-b412-4fa7-b234-a7a51d2b1291",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EDNet('pretrained/xlarge.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145d3586-3fa5-4d20-bc39-cb2d62a2cfb1",
   "metadata": {},
   "source": [
    "### 2. Evaluate the raw performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b2ad588-eb09-4c0d-b299-b2f80b0cb17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDNet 1.0 ✅ Python-3.9.19 ✅ torch-2.0.1 ✅CUDA:0 (NVIDIA A100 80GB PCIe MIG 7g.80gb, 81038MiB)\n",
      "ednet-x summary: 638 layers, 48,734,752 parameters, 0 gradients, 270.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/song/AOIUNO/datasets/VisDrone/VisDrone2019-DET-val/labels.cache... 548 images, 0 backgrounds, 0 corrupt: 100%|██████████| 548/548 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 35/35 [00:30<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759      0.583      0.475      0.502      0.314\n",
      "            pedestrian        520       8844      0.661      0.517      0.581      0.293\n",
      "                people        482       5125      0.607      0.427      0.468      0.204\n",
      "               bicycle        364       1287      0.353      0.265       0.25      0.119\n",
      "                   car        515      14064      0.786      0.832      0.865      0.638\n",
      "                   van        421       1975      0.596      0.515      0.538      0.393\n",
      "                 truck        266        750      0.591      0.407      0.456      0.313\n",
      "              tricycle        337       1045      0.522      0.376      0.379      0.222\n",
      "       awning-tricycle        220        532      0.324       0.19        0.2      0.128\n",
      "                   bus        131        251      0.773      0.637      0.692      0.533\n",
      "                 motor        485       4886      0.612      0.581      0.596      0.299\n",
      "Speed: 0.9ms preprocess, 33.4ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1mresults/xlarge/val3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = model.val(data='visdrone-det.yaml', split='val', project='results/xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eb582b-2dae-4748-bf22-9ac92beb36e0",
   "metadata": {},
   "source": [
    "### 3. Export the model - TensorRT\n",
    "\n",
    "The example GPU used here is NVIDIA A100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68f0d652-5b83-4110-999e-e570d16f6d1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ TensorRT requires GPU export, automatically assigning device=0\n",
      "EDNet 1.0 ✅ Python-3.9.19 ✅ torch-2.0.1 ✅CUDA:0 (NVIDIA A100 80GB PCIe MIG 7g.80gb, 81038MiB)\n",
      "ednet-x summary: 638 layers, 48,734,752 parameters, 0 gradients, 270.4 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'pretrained/xlarge.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 300, 6) (93.9 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.2 opset 17...\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 4.5s, saved as 'pretrained/xlarge.onnx' (180.6 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.2.0.post1...\n",
      "[08/24/2024-16:37:53] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 1411, GPU 1669 (MiB)\n",
      "[08/24/2024-16:37:54] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +1930, GPU +352, now: CPU 3341, GPU 2021 (MiB)\n",
      "[08/24/2024-16:37:54] [TRT] [I] ----------------------------------------------------------------\n",
      "[08/24/2024-16:37:54] [TRT] [I] Input filename:   pretrained/xlarge.onnx\n",
      "[08/24/2024-16:37:54] [TRT] [I] ONNX IR version:  0.0.8\n",
      "[08/24/2024-16:37:54] [TRT] [I] Opset version:    17\n",
      "[08/24/2024-16:37:54] [TRT] [I] Producer name:    pytorch\n",
      "[08/24/2024-16:37:54] [TRT] [I] Producer version: 2.0.1\n",
      "[08/24/2024-16:37:54] [TRT] [I] Domain:           \n",
      "[08/24/2024-16:37:54] [TRT] [I] Model version:    0\n",
      "[08/24/2024-16:37:54] [TRT] [I] Doc string:       \n",
      "[08/24/2024-16:37:54] [TRT] [I] ----------------------------------------------------------------\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 300, 6) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP32 engine as pretrained/xlarge.engine\n",
      "[08/24/2024-16:37:55] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[08/24/2024-16:40:38] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[08/24/2024-16:40:38] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[08/24/2024-16:40:40] [TRT] [I] Total Host Persistent Memory: 1256432\n",
      "[08/24/2024-16:40:40] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[08/24/2024-16:40:40] [TRT] [I] Total Scratch Memory: 8520704\n",
      "[08/24/2024-16:40:40] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 461 steps to complete.\n",
      "[08/24/2024-16:40:40] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 59.0626ms to assign 12 blocks to 461 nodes requiring 141641216 bytes.\n",
      "[08/24/2024-16:40:40] [TRT] [I] Total Activation Memory: 141640704\n",
      "[08/24/2024-16:40:40] [TRT] [I] Total Weights Memory: 189009604\n",
      "[08/24/2024-16:40:40] [TRT] [I] Engine generation completed in 165.126 seconds.\n",
      "[08/24/2024-16:40:40] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 14 MiB, GPU 573 MiB\n",
      "[08/24/2024-16:40:40] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 5556 MiB\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ✅ 173.0s, saved as 'pretrained/xlarge.engine' (186.6 MB)\n",
      "\n",
      "Export complete (173.7s)\n",
      "Results saved to \u001b[1m/home/song/UAV/pretrained\u001b[0m\n",
      "Predict:         yolo predict task=detect model=pretrained/xlarge.engine imgsz=640  \n",
      "Validate:        yolo val task=detect model=pretrained/xlarge.engine imgsz=640 data=visdrone-det.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pretrained/xlarge.engine'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.export(format='engine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1558333-127c-4dfc-a2f4-27f1f4388e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rt = EDNet('pretrained/xlarge.engine', task='detect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04d94fd2-8283-4e47-8e0c-1fa9679f56c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDNet 1.0 ✅ Python-3.9.19 ✅ torch-2.0.1 ✅CUDA:0 (NVIDIA A100 80GB PCIe MIG 7g.80gb, 81038MiB)\n",
      "Loading pretrained/xlarge.engine for TensorRT inference...\n",
      "[08/24/2024-16:56:06] [TRT] [I] Loaded engine size: 186 MiB\n",
      "[08/24/2024-16:56:06] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +135, now: CPU 1, GPU 315 (MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/song/AOIUNO/datasets/VisDrone/VisDrone2019-DET-val/labels.cache... 548 images, 0 backgrounds, 0 corrupt: 100%|██████████| 548/548 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 548/548 [00:13<00:00, 39.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759       0.59      0.471      0.502      0.314\n",
      "            pedestrian        520       8844      0.673      0.508       0.58      0.292\n",
      "                people        482       5125      0.611      0.426      0.468      0.203\n",
      "               bicycle        364       1287      0.366      0.261      0.251       0.12\n",
      "                   car        515      14064      0.788      0.831      0.863      0.637\n",
      "                   van        421       1975      0.595      0.511      0.539      0.393\n",
      "                 truck        266        750      0.612      0.404      0.457      0.314\n",
      "              tricycle        337       1045      0.529      0.371      0.377      0.222\n",
      "       awning-tricycle        220        532       0.32      0.186      0.197      0.126\n",
      "                   bus        131        251       0.79      0.633      0.692      0.535\n",
      "                 motor        485       4886      0.613      0.582      0.594      0.298\n",
      "Speed: 0.5ms preprocess, 8.5ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1mresults/xlarge/val4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = model_rt.val(data='visdrone-det.yaml', split='val', project='results/xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24c3e34-8d75-42b3-8bb1-051e31180868",
   "metadata": {},
   "source": [
    "### 4. Export the model for CPU Inference: Intel\n",
    "The example CPU used here is Intel Xeon Gold 6300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cebe4fbe-d7f2-4c92-af27-4ffb7b3f5771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDNet 1.0 ✅ Python-3.9.19 ✅ torch-2.0.1 ✅CPU (Intel Xeon Gold 6330 2.00GHz)\n",
      "ednet-n summary (fused): 394 layers, 2,871,712 parameters, 0 gradients, 15.2 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'pretrained/nano.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 300, 6) (6.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m starting export with openvino 2024.3.0-16041-1e3b88e4e3f-releases/2024/3...\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m export success ✅ 22.1s, saved as 'pretrained/nano_openvino_model/' (9.9 MB)\n",
      "\n",
      "Export complete (24.2s)\n",
      "Results saved to \u001b[1m/home/song/UAV/pretrained\u001b[0m\n",
      "Predict:         yolo predict task=detect model=pretrained/nano_openvino_model imgsz=640  \n",
      "Validate:        yolo val task=detect model=pretrained/nano_openvino_model imgsz=640 data=visdrone-det.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pretrained/nano_openvino_model'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EDNet('pretrained/nano.pt')\n",
    "model.export(format='openvino')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fddcc6a5-8c0d-43d1-a5e6-94de59660788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDNet 1.0 ✅ Python-3.9.19 ✅ torch-2.0.1 ✅CPU (Intel Xeon Gold 6330 2.00GHz)\n",
      "Loading pretrained/nano_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "Setting batch=1 input of shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/song/AOIUNO/datasets/VisDrone/VisDrone2019-DET-val/labels.cache... 548 images, 0 backgrounds, 0 corrupt: 100%|██████████| 548/548 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 548/548 [01:02<00:00,  8.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759      0.449      0.344      0.341      0.199\n",
      "            pedestrian        520       8844      0.499      0.379      0.403      0.184\n",
      "                people        482       5125      0.446       0.29      0.295      0.118\n",
      "               bicycle        364       1287      0.241      0.111     0.0908     0.0367\n",
      "                   car        515      14064      0.654      0.778      0.786      0.544\n",
      "                   van        421       1975      0.462      0.381      0.382      0.264\n",
      "                 truck        266        750      0.431      0.258      0.251      0.156\n",
      "              tricycle        337       1045       0.38      0.206      0.201      0.109\n",
      "       awning-tricycle        220        532      0.246      0.148       0.11     0.0723\n",
      "                   bus        131        251      0.648      0.462      0.489      0.327\n",
      "                 motor        485       4886      0.484      0.424      0.402       0.18\n",
      "Speed: 0.5ms preprocess, 79.9ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mresults/nano/val\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_intel = EDNet('pretrained/nano_openvino_model', task='detect')\n",
    "results = model_intel.val(data='visdrone-det.yaml', split='val', device='cpu', project='results/nano')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00618144",
   "metadata": {},
   "source": [
    "### 5. Export the model for CPU Inference: ARM\n",
    "The example CPU used here is ARMv8 Firestorm (performance cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e387e20-5cac-475b-bd0d-92f4803e01f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDNet 1.0 ✅ Python-3.9.19 ✅ torch-2.0.1 ✅CPU (Apple M1)\n",
      "ednet-t summary (fused): 366 layers, 1,781,088 parameters, 0 gradients, 14.0 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'pretrained/tiny.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 300, 6) (3.9 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.2 opset 17...\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 1.3s, saved as 'pretrained/tiny.onnx' (6.0 MB)\n",
      "\n",
      "Export complete (2.0s)\n",
      "Results saved to \u001b[1m/Users/zhifansong/Desktop/EdgeDroneNet/UAV/pretrained\u001b[0m\n",
      "Predict:         yolo predict task=detect model=pretrained/tiny.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=pretrained/tiny.onnx imgsz=640 data=visdrone-det.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pretrained/tiny.onnx'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EDNet('pretrained/tiny.pt')\n",
    "model.export(format='onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f66beb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDNet 1.0 ✅ Python-3.9.19 ✅ torch-2.0.1 ✅CPU (Apple M1)\n",
      "Loading pretrained/tiny.onnx for ONNX Runtime inference...\n",
      "Setting batch=1 input of shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/zhifansong/Desktop/EdgeDroneNet/EdgeDroneNet/datasets/VisDrone/VisDrone2019-DET-val/labels.cache... 548 images, 0 backgrounds, 0 corrupt: 100%|██████████| 548/548 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 548/548 [00:39<00:00, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759      0.423      0.341      0.332      0.195\n",
      "            pedestrian        520       8844       0.48      0.369      0.391      0.179\n",
      "                people        482       5125      0.437      0.295      0.293      0.116\n",
      "               bicycle        364       1287      0.229      0.127     0.0982     0.0426\n",
      "                   car        515      14064      0.635      0.782      0.783      0.541\n",
      "                   van        421       1975      0.459      0.363      0.373      0.259\n",
      "                 truck        266        750       0.37       0.26      0.233      0.149\n",
      "              tricycle        337       1045      0.383      0.202      0.201       0.11\n",
      "       awning-tricycle        220        532       0.23      0.128      0.108     0.0681\n",
      "                   bus        131        251      0.539       0.45       0.43        0.3\n",
      "                 motor        485       4886      0.469       0.43      0.405      0.182\n",
      "Speed: 0.6ms preprocess, 62.3ms inference, 0.0ms loss, 0.1ms postprocess per image\n",
      "Results saved to \u001b[1mresults/tiny/val\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_arm = EDNet('pretrained/tiny.onnx', task='detect')\n",
    "results = model_arm.val(data='visdrone-det.yaml', split='val', project='results/tiny')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
